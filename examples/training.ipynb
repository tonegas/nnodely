{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNodely Documentation - Training\n",
    "\n",
    "Here are listed all the modalities that can be used to perform the training process of a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnodely\n",
      "  Downloading nnodely-0.14.3-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy==1.26.4 (from nnodely)\n",
      "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting onnx==1.16.2 (from nnodely)\n",
      "  Downloading onnx-1.16.2.tar.gz (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas==2.2.2 (from nnodely)\n",
      "  Downloading pandas-2.2.2.tar.gz (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of nnodely to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nnodely\n",
      "  Downloading nnodely-0.14.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading nnodely-0.14.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy (from nnodely)\n",
      "  Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting onnx (from nnodely)\n",
      "  Downloading onnx-1.18.0-cp313-cp313-macosx_12_0_universal2.whl.metadata (6.9 kB)\n",
      "Collecting pandas (from nnodely)\n",
      "  Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting torch (from nnodely)\n",
      "  Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting reportlab (from nnodely)\n",
      "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting matplotlib (from nnodely)\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->nnodely)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->nnodely)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->nnodely)\n",
      "  Using cached fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->nnodely)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/giovanni/Desktop/Projects/nnodely/venv/lib/python3.13/site-packages (from matplotlib->nnodely) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib->nnodely)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->nnodely)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/giovanni/Desktop/Projects/nnodely/venv/lib/python3.13/site-packages (from matplotlib->nnodely) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/giovanni/Desktop/Projects/nnodely/venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->nnodely) (1.17.0)\n",
      "Collecting protobuf>=4.25.1 (from onnx->nnodely)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting typing_extensions>=4.7.1 (from onnx->nnodely)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nnodely)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nnodely)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer (from reportlab->nnodely)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting filelock (from torch->nnodely)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch->nnodely)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch->nnodely)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->nnodely)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->nnodely)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch->nnodely)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->nnodely)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->nnodely)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading nnodely-0.14.1-py3-none-any.whl (71 kB)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading onnx-1.18.0-cp313-cp313-macosx_12_0_universal2.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: pytz, mpmath, tzdata, typing_extensions, sympy, setuptools, pyparsing, protobuf, pillow, numpy, networkx, MarkupSafe, kiwisolver, fsspec, fonttools, filelock, cycler, charset-normalizer, reportlab, pandas, onnx, jinja2, contourpy, torch, matplotlib, nnodely\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [nnodely]5/26\u001b[0m [nnodely]ib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 charset-normalizer-3.4.2 contourpy-1.3.3 cycler-0.12.1 filelock-3.18.0 fonttools-4.59.0 fsspec-2025.7.0 jinja2-3.1.6 kiwisolver-1.4.8 matplotlib-3.10.5 mpmath-1.3.0 networkx-3.5 nnodely-0.14.1 numpy-2.3.2 onnx-1.18.0 pandas-2.3.1 pillow-11.3.0 protobuf-6.31.1 pyparsing-3.2.3 pytz-2025.2 reportlab-4.4.3 setuptools-80.9.0 sympy-1.14.0 torch-2.7.1 typing_extensions-4.14.1 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>- nnodely_v0.14.1 --<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "# uncomment the command below to install the nnodely package\n",
    "#!pip install nnodely\n",
    "\n",
    "from nnodely import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m================================ nnodely Model =================================\u001b[0m\n",
      "\u001b[32m{'Constants': {},\n",
      " 'Functions': {},\n",
      " 'Info': {'SampleTime': 0.01, 'ns': [5, 0], 'ntot': 5},\n",
      " 'Inputs': {'in1': {'dim': 1,\n",
      "                    'ns': [5, 0],\n",
      "                    'ntot': 5,\n",
      "                    'sw': [0, 0],\n",
      "                    'tw': [-0.05, 0]},\n",
      "            'target': {'dim': 1,\n",
      "                       'ns': [1, 0],\n",
      "                       'ntot': 1,\n",
      "                       'sw': [-1, 0],\n",
      "                       'tw': [0, 0]}},\n",
      " 'Minimizers': {'error': {'A': 'out', 'B': 'SamplePart7', 'loss': 'mse'}},\n",
      " 'Models': 'model',\n",
      " 'Outputs': {'out': 'Fir4'},\n",
      " 'Parameters': {'PFir2p': {'dim': 1, 'tw': 0.05}},\n",
      " 'Relations': {'Fir4': ['Fir', ['TimePart3'], 'PFir2p', None, 0],\n",
      "               'SamplePart7': ['SamplePart', ['target'], [-1, 0]],\n",
      "               'TimePart3': ['TimePart', ['in1'], [-0.05, 0]]},\n",
      " 'States': {}}\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Model Dataset =============================\u001b[0m\n",
      "\u001b[32mDataset Name:                 dataset\u001b[0m\n",
      "\u001b[32mNumber of files:              1\u001b[0m\n",
      "\u001b[32mTotal number of samples:      27\u001b[0m\n",
      "\u001b[32mShape of target:              (27, 1, 1)\u001b[0m\n",
      "\u001b[32mShape of in1:                 (27, 5, 1)\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Create a neural network model and Load a dataset\n",
    "in1 = Input('in1')\n",
    "target = Input('target')\n",
    "relation = Fir(in1.tw(0.05))\n",
    "output = Output('out', relation)\n",
    "\n",
    "model = Modely(visualizer=TextVisualizer())\n",
    "model.addMinimize('error', output, target.last())\n",
    "model.addModel('model', output)\n",
    "model.neuralizeModel(0.01)\n",
    "\n",
    "train_folder = 'data'\n",
    "data_struct = ['in1', '', 'target']\n",
    "model.loadData(name='dataset', source=train_folder, format=data_struct, skiplines=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parameters and Usage\n",
    "\n",
    "With the `trainModel` function the user can perform a training process by setting different parameters of training such as:\n",
    "\n",
    "- models to train\n",
    "- training and validation dataset\n",
    "- number of epochs of training\n",
    "- data shuffling\n",
    "- train and validation batch size\n",
    "- learning rate\n",
    "\n",
    "And many more advanced settings.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no parameters are specified, the function will train all the built models using all the loaded dataset with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                100\u001b[0m\n",
      "\u001b[32mupdate per epochs:            1.0\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size)/batch_size+1\u001b[0m\n",
      "\u001b[32mshuffle data:                 True\u001b[0m\n",
      "\u001b[32mtrain dataset:                train_dataset_0.70\u001b[0m\n",
      "\u001b[32m\t- num of samples:            19\u001b[0m\n",
      "\u001b[32m\t- batch size:                19\u001b[0m\n",
      "\u001b[32m\t- unused samples:            0.0\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-update_per_epochs*batch_size\u001b[0m\n",
      "\u001b[32mval dataset:                  validation_dataset_0.20\u001b[0m\n",
      "\u001b[32mval {batch size, samples}:    {5, 5}\u001b[0m\n",
      "\u001b[32mtest dataset:                 test_dataset_0.10\u001b[0m\n",
      "\u001b[32mtest {batch size, samples}:   {3, 3}\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'out',\n",
      "                                         'B': 'SamplePart7',\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.001}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir2p'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m  train  |\u001b[0m\u001b[32m   val   |\u001b[0m\u001b[32m  train  |\u001b[0m\u001b[32m   val   |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|  10/100  |\u001b[0m\u001b[32m6.510e+02|\u001b[0m\u001b[32m2.514e+03|\u001b[0m\u001b[32m6.510e+02|\u001b[0m\u001b[32m2.514e+03|\u001b[0m\n",
      "\u001b[32m|  20/100  |\u001b[0m\u001b[32m6.202e+02|\u001b[0m\u001b[32m2.400e+03|\u001b[0m\u001b[32m6.202e+02|\u001b[0m\u001b[32m2.400e+03|\u001b[0m\n",
      "\u001b[32m|  30/100  |\u001b[0m\u001b[32m5.904e+02|\u001b[0m\u001b[32m 2.29e+03|\u001b[0m\u001b[32m5.904e+02|\u001b[0m\u001b[32m 2.29e+03|\u001b[0m\n",
      "\u001b[32m|  40/100  |\u001b[0m\u001b[32m5.616e+02|\u001b[0m\u001b[32m2.183e+03|\u001b[0m\u001b[32m5.616e+02|\u001b[0m\u001b[32m2.183e+03|\u001b[0m\n",
      "\u001b[32m|  50/100  |\u001b[0m\u001b[32m5.339e+02|\u001b[0m\u001b[32m2.081e+03|\u001b[0m\u001b[32m5.339e+02|\u001b[0m\u001b[32m2.081e+03|\u001b[0m\n",
      "\u001b[32m|  60/100  |\u001b[0m\u001b[32m5.072e+02|\u001b[0m\u001b[32m1.981e+03|\u001b[0m\u001b[32m5.072e+02|\u001b[0m\u001b[32m1.981e+03|\u001b[0m\n",
      "\u001b[32m|  70/100  |\u001b[0m\u001b[32m4.815e+02|\u001b[0m\u001b[32m1.886e+03|\u001b[0m\u001b[32m4.815e+02|\u001b[0m\u001b[32m1.886e+03|\u001b[0m\n",
      "\u001b[32m|  80/100  |\u001b[0m\u001b[32m4.568e+02|\u001b[0m\u001b[32m1.794e+03|\u001b[0m\u001b[32m4.568e+02|\u001b[0m\u001b[32m1.794e+03|\u001b[0m\n",
      "\u001b[32m|  90/100  |\u001b[0m\u001b[32m4.331e+02|\u001b[0m\u001b[32m1.706e+03|\u001b[0m\u001b[32m4.331e+02|\u001b[0m\u001b[32m1.706e+03|\u001b[0m\n",
      "\u001b[32m| 100/100  |\u001b[0m\u001b[32m4.103e+02|\u001b[0m\u001b[32m1.621e+03|\u001b[0m\u001b[32m4.103e+02|\u001b[0m\u001b[32m1.621e+03|\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.0738527774810791\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[34mThe selected model is the LAST model of the training.\u001b[0m\n",
      "\u001b[1;32m=========== nnodely Model Results for train_dataset_0.70 ==========\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     4.081e+02     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m     2.052e+02     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     4.081e+02     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m     2.052e+02     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m======== nnodely Model Results for validation_dataset_0.20 ========\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     1.621e+03     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m     1.144e+03     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     1.621e+03     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m     1.144e+03     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m=========== nnodely Model Results for test_dataset_0.10 ===========\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     2.287e+03     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m      2.85e+03     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     2.287e+03     |\u001b[0m\u001b[32m     2.032e+00     |\u001b[0m\u001b[32m      2.85e+03     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_parameters = model.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       'model'\u001b[0m\n",
      "\u001b[32mnum of epochs:                10\u001b[0m\n",
      "\u001b[32mupdate per epochs:            6.75\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size)/batch_size+1\u001b[0m\n",
      "\u001b[32mshuffle data:                 True\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            27\u001b[0m\n",
      "\u001b[32m\t- batch size:                4\u001b[0m\n",
      "\u001b[32m\t- unused samples:            0.0\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-update_per_epochs*batch_size\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'out',\n",
      "                                         'B': 'SamplePart7',\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.01}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir2p'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m       train       |\u001b[0m\u001b[32m       train       |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|   1/10   |\u001b[0m\u001b[32m     7.389e+02     |\u001b[0m\u001b[32m     7.389e+02     |\u001b[0m\n",
      "\u001b[32m|   2/10   |\u001b[0m\u001b[32m     4.521e+02     |\u001b[0m\u001b[32m     4.521e+02     |\u001b[0m\n",
      "\u001b[32m|   3/10   |\u001b[0m\u001b[32m     3.187e+02     |\u001b[0m\u001b[32m     3.187e+02     |\u001b[0m\n",
      "\u001b[32m|   4/10   |\u001b[0m\u001b[32m     1.909e+02     |\u001b[0m\u001b[32m     1.909e+02     |\u001b[0m\n",
      "\u001b[32m|   5/10   |\u001b[0m\u001b[32m     9.889e+01     |\u001b[0m\u001b[32m     9.889e+01     |\u001b[0m\n",
      "\u001b[32m|   6/10   |\u001b[0m\u001b[32m     4.236e+01     |\u001b[0m\u001b[32m     4.236e+01     |\u001b[0m\n",
      "\u001b[32m|   7/10   |\u001b[0m\u001b[32m     1.896e+01     |\u001b[0m\u001b[32m     1.896e+01     |\u001b[0m\n",
      "\u001b[32m|   8/10   |\u001b[0m\u001b[32m     6.181e+00     |\u001b[0m\u001b[32m     6.181e+00     |\u001b[0m\n",
      "\u001b[32m|   9/10   |\u001b[0m\u001b[32m     3.443e+00     |\u001b[0m\u001b[32m     3.443e+00     |\u001b[0m\n",
      "\u001b[32m|  10/10   |\u001b[0m\u001b[32m     3.040e+00     |\u001b[0m\u001b[32m     3.040e+00     |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.028616905212402344\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[34mThe selected model is the LAST model of the training.\u001b[0m\n",
      "\u001b[1;32m================ nnodely Model Results for dataset ================\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     4.007e+00     |\u001b[0m\u001b[32m     1.828e-02     |\u001b[0m\u001b[32m     1.508e+02     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     4.007e+00     |\u001b[0m\u001b[32m     1.828e-02     |\u001b[0m\u001b[32m     1.508e+02     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_parameters = model.trainModel(models='model', train_dataset='dataset', num_of_epochs=10, lr=0.01, shuffle_data=True, train_batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically splits the dataset between training set and validation set, just provide the datasets to use inside the `dataset` parameter.\n",
    "\n",
    "In this way, the parameter `splits` will be used to splits the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Modely.trainModel() got an unexpected keyword argument 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_parameters = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Modely.trainModel() got an unexpected keyword argument 'dataset'"
     ]
    }
   ],
   "source": [
    "training_parameters = model.trainModel(models='model', dataset='dataset', splits=[70, 20, 10], num_of_epochs=10, shuffle_data=True, train_batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Training\n",
    "\n",
    "The recurrent training can take place only when there are recurrent variables, closed-loops or connections between inputs.\n",
    "\n",
    "The recurrent train can also be performed on the run by setting the `closed_loop` dictionary for closed connections and/or by setting the `connect` dictionary for direct connections.\n",
    "\n",
    "In case of a recurrent training, the number of prediction horizon window (`prediction_samples`) must be specified. This is used to select for how many steps doing the recurrent loop\n",
    "\n",
    "The `step` is used to decide whether to skip samples at each epoch. this will ensure a faster training time expecially when the prediction horizon is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[check_names] The name 'target' is already in defined as NeuObj but it is overwritten.\u001b[0m\n",
      "\u001b[33m[check_names] The name 'out' is already in defined as NeuObj but it is overwritten.\u001b[0m\n",
      "\u001b[1;32m================================ nnodely Model =================================\u001b[0m\n",
      "\u001b[32m{'Constants': {},\n",
      " 'Functions': {},\n",
      " 'Info': {'SampleTime': 0.01,\n",
      "          'nnodely_version': '1.5.0',\n",
      "          'ns': [1, 1],\n",
      "          'ntot': 2,\n",
      "          'num_parameters': 1},\n",
      " 'Inputs': {'target': {'dim': 1, 'ns': [0, 1], 'ntot': 1, 'sw': [0, 1]},\n",
      "            'x': {'closedLoop': 'Fir7',\n",
      "                  'dim': 1,\n",
      "                  'local': 1,\n",
      "                  'ns': [1, 0],\n",
      "                  'ntot': 1,\n",
      "                  'sw': [-1, 0]}},\n",
      " 'Minimizers': {'error': {'A': 'SamplePart9', 'B': 'Fir7', 'loss': 'mse'}},\n",
      " 'Models': 'model',\n",
      " 'Outputs': {'out': 'Fir7'},\n",
      " 'Parameters': {'PFir8W': {'dim': 1,\n",
      "                           'sw': 1,\n",
      "                           'values': [[0.8822692632675171]]}},\n",
      " 'Relations': {'Fir7': ['Fir', ['SamplePart6'], 'PFir8W', None, 0],\n",
      "               'SamplePart6': ['SamplePart', ['x'], -1, [-1, 0]],\n",
      "               'SamplePart9': ['SamplePart', ['target'], -1, [0, 1]]}}\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Model Dataset =============================\u001b[0m\n",
      "\u001b[32mDataset Name:                 dataset\u001b[0m\n",
      "\u001b[32mNumber of files:              1\u001b[0m\n",
      "\u001b[32mTotal number of samples:      19\u001b[0m\n",
      "\u001b[32mShape of x:                   (19, 1, 1)\u001b[0m\n",
      "\u001b[32mShape of target:              (19, 1, 1)\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "target = Input('target')\n",
    "x = Input('x')\n",
    "relation = Fir(x.last())\n",
    "relation.closedLoop(x)\n",
    "output = Output('out', relation)\n",
    "\n",
    "test = Modely(visualizer=TextVisualizer(), seed=42)\n",
    "test.addModel('model', output)\n",
    "test.addMinimize('error', target.next(), relation)\n",
    "test.neuralizeModel(0.01)\n",
    "\n",
    "dataset = {'x': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], 'target': [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]}\n",
    "test.loadData(name='dataset', source=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                10\u001b[0m\n",
      "\u001b[32mupdate per epochs:            3\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size-prediction_samples+1)/(batch_size+step-1)+1\u001b[0m\n",
      "\u001b[32mshuffle _data:                True\u001b[0m\n",
      "\u001b[32mprediction samples:           2\u001b[0m\n",
      "\u001b[32mstep:                         1\u001b[0m\n",
      "\u001b[32mclosed loop:                  {}\u001b[0m\n",
      "\u001b[32mconnect:                      {}\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            19\u001b[0m\n",
      "\u001b[32m\t- batch size:                4\u001b[0m\n",
      "\u001b[32m\t- unused samples:            5\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-prediction_samples-update_per_epochs*(batch_size+step-1)\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'SamplePart9',\n",
      "                                         'B': 'Fir7',\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.01}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir8W'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m       train       |\u001b[0m\u001b[32m       train       |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|   1/10   |\u001b[0m\u001b[32m     4.213e+02     |\u001b[0m\u001b[32m     4.213e+02     |\u001b[0m\n",
      "\u001b[32m|   2/10   |\u001b[0m\u001b[32m     4.120e+02     |\u001b[0m\u001b[32m     4.120e+02     |\u001b[0m\n",
      "\u001b[32m|   3/10   |\u001b[0m\u001b[32m     3.957e+02     |\u001b[0m\u001b[32m     3.957e+02     |\u001b[0m\n",
      "\u001b[32m|   4/10   |\u001b[0m\u001b[32m     3.765e+02     |\u001b[0m\u001b[32m     3.765e+02     |\u001b[0m\n",
      "\u001b[32m|   5/10   |\u001b[0m\u001b[32m      3.58e+02     |\u001b[0m\u001b[32m      3.58e+02     |\u001b[0m\n",
      "\u001b[32m|   6/10   |\u001b[0m\u001b[32m     3.395e+02     |\u001b[0m\u001b[32m     3.395e+02     |\u001b[0m\n",
      "\u001b[32m|   7/10   |\u001b[0m\u001b[32m     3.259e+02     |\u001b[0m\u001b[32m     3.259e+02     |\u001b[0m\n",
      "\u001b[32m|   8/10   |\u001b[0m\u001b[32m     2.979e+02     |\u001b[0m\u001b[32m     2.979e+02     |\u001b[0m\n",
      "\u001b[32m|   9/10   |\u001b[0m\u001b[32m      2.89e+02     |\u001b[0m\u001b[32m      2.89e+02     |\u001b[0m\n",
      "\u001b[32m|  10/10   |\u001b[0m\u001b[32m     2.871e+02     |\u001b[0m\u001b[32m     2.871e+02     |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.039998769760131836\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_parameters = test.trainModel(train_dataset='dataset', lr=0.01, num_of_epochs=10, train_batch_size=4, prediction_samples=2, step=1, shuffle_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set different weight for the minimization functions\n",
    "\n",
    "use the `minimize_gain` attribute to modify the importance of certain minimization functions by passing a dictionary with the gain factor for each minimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                100\u001b[0m\n",
      "\u001b[32mupdate per epochs:            1\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size-prediction_samples+1)/(batch_size+step-1)+1\u001b[0m\n",
      "\u001b[32mshuffle _data:                True\u001b[0m\n",
      "\u001b[32mprediction samples:           0\u001b[0m\n",
      "\u001b[32mstep:                         0\u001b[0m\n",
      "\u001b[32mclosed loop:                  {}\u001b[0m\n",
      "\u001b[32mconnect:                      {}\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            19\u001b[0m\n",
      "\u001b[32m\t- batch size:                19\u001b[0m\n",
      "\u001b[32m\t- unused samples:            0\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-prediction_samples-update_per_epochs*(batch_size+step-1)\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'SamplePart9',\n",
      "                                         'B': 'Fir7',\n",
      "                                         'gain': 0,\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.001}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir8W'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m       train       |\u001b[0m\u001b[32m       train       |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|  10/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  20/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  30/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  40/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  50/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  60/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  70/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  80/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  90/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m| 100/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.08199954032897949\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_parameters = test.trainModel(train_dataset='dataset', minimize_gain={'error':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Optimizer\n",
    "\n",
    "There are various ways to configure the optimizer to use during the training process.\n",
    "\n",
    "This can be achieved by selecting the name of the optimizer to use from the pytorch list and then passing a dictionary of options to configure the desired optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                10\u001b[0m\n",
      "\u001b[32mupdate per epochs:            1\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size-prediction_samples+1)/(batch_size+step-1)+1\u001b[0m\n",
      "\u001b[32mshuffle _data:                True\u001b[0m\n",
      "\u001b[32mprediction samples:           0\u001b[0m\n",
      "\u001b[32mstep:                         0\u001b[0m\n",
      "\u001b[32mclosed loop:                  {}\u001b[0m\n",
      "\u001b[32mconnect:                      {}\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            19\u001b[0m\n",
      "\u001b[32m\t- batch size:                19\u001b[0m\n",
      "\u001b[32m\t- unused samples:            0\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-prediction_samples-update_per_epochs*(batch_size+step-1)\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'SamplePart9',\n",
      "                                         'B': 'Fir7',\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'betas': (0.5, 0.99), 'lr': 0.1}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir8W'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m       train       |\u001b[0m\u001b[32m       train       |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|   1/10   |\u001b[0m\u001b[32m     3.712e+02     |\u001b[0m\u001b[32m     3.712e+02     |\u001b[0m\n",
      "\u001b[32m|   2/10   |\u001b[0m\u001b[32m     3.350e+02     |\u001b[0m\u001b[32m     3.350e+02     |\u001b[0m\n",
      "\u001b[32m|   3/10   |\u001b[0m\u001b[32m     3.019e+02     |\u001b[0m\u001b[32m     3.019e+02     |\u001b[0m\n",
      "\u001b[32m|   4/10   |\u001b[0m\u001b[32m     2.719e+02     |\u001b[0m\u001b[32m     2.719e+02     |\u001b[0m\n",
      "\u001b[32m|   5/10   |\u001b[0m\u001b[32m     2.451e+02     |\u001b[0m\u001b[32m     2.451e+02     |\u001b[0m\n",
      "\u001b[32m|   6/10   |\u001b[0m\u001b[32m     2.215e+02     |\u001b[0m\u001b[32m     2.215e+02     |\u001b[0m\n",
      "\u001b[32m|   7/10   |\u001b[0m\u001b[32m     2.008e+02     |\u001b[0m\u001b[32m     2.008e+02     |\u001b[0m\n",
      "\u001b[32m|   8/10   |\u001b[0m\u001b[32m     1.830e+02     |\u001b[0m\u001b[32m     1.830e+02     |\u001b[0m\n",
      "\u001b[32m|   9/10   |\u001b[0m\u001b[32m     1.678e+02     |\u001b[0m\u001b[32m     1.678e+02     |\u001b[0m\n",
      "\u001b[32m|  10/10   |\u001b[0m\u001b[32m     1.549e+02     |\u001b[0m\u001b[32m     1.549e+02     |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.009000539779663086\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer_defaults = {\n",
    "            'lr': 0.1,\n",
    "            'betas': (0.5, 0.99)\n",
    "        }\n",
    "\n",
    "training_parameters = test.trainModel(train_dataset='dataset', optimizer='Adam', optimizer_defaults=optimizer_defaults, num_of_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping and Best Model\n",
    "\n",
    "Use one of the built-in early stopping and selection model functions or use a custom one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                100\u001b[0m\n",
      "\u001b[32mupdate per epochs:            1\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size-prediction_samples+1)/(batch_size+step-1)+1\u001b[0m\n",
      "\u001b[32mshuffle _data:                True\u001b[0m\n",
      "\u001b[32mearly stopping:               early_stop_patience\u001b[0m\n",
      "\u001b[32mearly stopping params:        {}\u001b[0m\n",
      "\u001b[32mprediction samples:           0\u001b[0m\n",
      "\u001b[32mstep:                         0\u001b[0m\n",
      "\u001b[32mclosed loop:                  {}\u001b[0m\n",
      "\u001b[32mconnect:                      {}\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            19\u001b[0m\n",
      "\u001b[32m\t- batch size:                19\u001b[0m\n",
      "\u001b[32m\t- unused samples:            0\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-prediction_samples-update_per_epochs*(batch_size+step-1)\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'SamplePart9',\n",
      "                                         'B': 'Fir7',\n",
      "                                         'gain': 0,\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.001}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir8W'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m       train       |\u001b[0m\u001b[32m       train       |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|  10/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  20/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  30/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "|  32/100  |       0.e+00      |       0.e+00      |"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m|  40/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[32m|  50/100  |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\u001b[32m       0.e+00      |\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.07100605964660645\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nnodely.support.earlystopping import early_stop_patience, select_best_model\n",
    "training_parameters = test.trainModel(train_dataset='dataset', minimize_gain={'error':0}, early_stopping=early_stop_patience, select_model=select_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Analyze\n",
    "\n",
    "the function `trainAndAnalyze` is a convenient way to do the training and validation at the same time. \n",
    "\n",
    "With the following function the user can use a test dataset to assess the quality of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m============================ nnodely Model Dataset =============================\u001b[0m\n",
      "\u001b[32mDataset Name:                 dataset_test\u001b[0m\n",
      "\u001b[32mNumber of files:              1\u001b[0m\n",
      "\u001b[32mTotal number of samples:      27\u001b[0m\n",
      "\u001b[32mShape of in1:                 (27, 5, 1)\u001b[0m\n",
      "\u001b[32mShape of target:              (27, 1, 1)\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Model Dataset =============================\u001b[0m\n",
      "\u001b[32mDataset Name:                 dataset_val\u001b[0m\n",
      "\u001b[32mNumber of files:              1\u001b[0m\n",
      "\u001b[32mTotal number of samples:      27\u001b[0m\n",
      "\u001b[32mShape of in1:                 (27, 5, 1)\u001b[0m\n",
      "\u001b[32mShape of target:              (27, 1, 1)\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m======================== nnodely Model Train Parameters ========================\u001b[0m\n",
      "\u001b[32mmodels:                       ['model']\u001b[0m\n",
      "\u001b[32mnum of epochs:                10\u001b[0m\n",
      "\u001b[32mupdate per epochs:            6\u001b[0m\n",
      "\u001b[34m└>(n_samples-batch_size)/batch_size+1\u001b[0m\n",
      "\u001b[32mshuffle _data:                True\u001b[0m\n",
      "\u001b[32mtrain dataset:                dataset\u001b[0m\n",
      "\u001b[32m\t- num of samples:            27\u001b[0m\n",
      "\u001b[32m\t- batch size:                4\u001b[0m\n",
      "\u001b[32m\t- unused samples:            3\u001b[0m\n",
      "\u001b[34m\t  └>n_samples-update_per_epochs*batch_size\u001b[0m\n",
      "\u001b[32mval dataset:                  dataset_val\u001b[0m\n",
      "\u001b[32mval {batch size, samples}:    {27, 27}\u001b[0m\n",
      "\u001b[32mminimizers:                   {'error': {'A': 'Fir2',\n",
      "                                         'B': 'SamplePart4',\n",
      "                                         'loss': 'mse'}}\u001b[0m\n",
      "\u001b[32moptimizer:                    Adam\u001b[0m\n",
      "\u001b[32moptimizer defaults:           {'lr': 0.01}\u001b[0m\n",
      "\u001b[32moptimizer params:             [{'params': 'PFir3W'}]\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================= nnodely Training =================\u001b[0m\n",
      "\u001b[32m|  Epoch   |\u001b[0m\u001b[32m       error       |\u001b[0m\u001b[32m       Total       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m        Loss       |\u001b[0m\u001b[32m        Loss       |\u001b[0m\n",
      "\u001b[32m|          |\u001b[0m\u001b[32m  train  |\u001b[0m\u001b[32m   val   |\u001b[0m\u001b[32m  train  |\u001b[0m\u001b[32m   val   |\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|   1/10   |\u001b[0m\u001b[32m1.929e+00|\u001b[0m\u001b[32m1.635e+00|\u001b[0m\u001b[32m1.929e+00|\u001b[0m\u001b[32m1.635e+00|\u001b[0m\n",
      "\u001b[32m|   2/10   |\u001b[0m\u001b[32m1.961e+00|\u001b[0m\u001b[32m1.623e+00|\u001b[0m\u001b[32m1.961e+00|\u001b[0m\u001b[32m1.623e+00|\u001b[0m\n",
      "\u001b[32m|   3/10   |\u001b[0m\u001b[32m1.617e+00|\u001b[0m\u001b[32m1.776e+00|\u001b[0m\u001b[32m1.617e+00|\u001b[0m\u001b[32m1.776e+00|\u001b[0m\n",
      "\u001b[32m|   4/10   |\u001b[0m\u001b[32m1.671e+00|\u001b[0m\u001b[32m1.432e+00|\u001b[0m\u001b[32m1.671e+00|\u001b[0m\u001b[32m1.432e+00|\u001b[0m\n",
      "\u001b[32m|   5/10   |\u001b[0m\u001b[32m1.603e+00|\u001b[0m\u001b[32m 1.41e+00|\u001b[0m\u001b[32m1.603e+00|\u001b[0m\u001b[32m 1.41e+00|\u001b[0m\n",
      "\u001b[32m|   6/10   |\u001b[0m\u001b[32m1.623e+00|\u001b[0m\u001b[32m1.408e+00|\u001b[0m\u001b[32m1.623e+00|\u001b[0m\u001b[32m1.408e+00|\u001b[0m\n",
      "\u001b[32m|   7/10   |\u001b[0m\u001b[32m1.455e+00|\u001b[0m\u001b[32m1.406e+00|\u001b[0m\u001b[32m1.455e+00|\u001b[0m\u001b[32m1.406e+00|\u001b[0m\n",
      "\u001b[32m|   8/10   |\u001b[0m\u001b[32m1.403e+00|\u001b[0m\u001b[32m1.358e+00|\u001b[0m\u001b[32m1.403e+00|\u001b[0m\u001b[32m1.358e+00|\u001b[0m\n",
      "\u001b[32m|   9/10   |\u001b[0m\u001b[32m1.495e+00|\u001b[0m\u001b[32m1.288e+00|\u001b[0m\u001b[32m1.495e+00|\u001b[0m\u001b[32m1.288e+00|\u001b[0m\n",
      "\u001b[32m|  10/10   |\u001b[0m\u001b[32m1.386e+00|\u001b[0m\u001b[32m1.249e+00|\u001b[0m\u001b[32m1.386e+00|\u001b[0m\u001b[32m1.249e+00|\u001b[0m\n",
      "\u001b[32m|--------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============================ nnodely Training Time =============================\u001b[0m\n",
      "\u001b[32mTotal time of Training:       0.036997079849243164\u001b[0m\n",
      "\u001b[32m================================================================================\u001b[0m\n",
      "\u001b[1;32m================ nnodely Model Results for dataset ================\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m      1.27e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     8.712e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m      1.27e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     8.712e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============== nnodely Model Results for dataset_val ==============\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     1.249e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     9.337e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     1.249e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     9.337e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[1;32m============== nnodely Model Results for dataset_test =============\u001b[0m\n",
      "\u001b[32m| Loss|\u001b[0m\u001b[32m        mse        |\u001b[0m\u001b[32m        FVU        |\u001b[0m\u001b[32m        AIC        |\u001b[0m\n",
      "\u001b[32m|     |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    small better   |\u001b[0m\u001b[32m    lower better   |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|error|\u001b[0m\u001b[32m     1.249e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     9.337e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n",
      "\u001b[32m|Total|\u001b[0m\u001b[32m     1.249e+00     |\u001b[0m\u001b[32m     1.464e-02     |\u001b[0m\u001b[32m     9.337e+01     |\u001b[0m\n",
      "\u001b[32m|-----------------------------------------------------------------|\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_folder = 'data'\n",
    "data_struct = ['in1', '', 'target']\n",
    "model.loadData(name='dataset_test', source=test_folder, format=data_struct, skiplines=1)\n",
    "model.loadData(name='dataset_val', source=test_folder, format=data_struct, skiplines=1)\n",
    "\n",
    "training_parameters = model.trainAndAnalyze(models='model', train_dataset='dataset', validation_dataset='dataset_val', train_batch_size=4, test_dataset='dataset_test', test_batch_size=1, num_of_epochs=10, lr=0.01, shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add_optimizer_defaults': {},\n",
      " 'add_optimizer_params': [],\n",
      " 'all_models': ['model'],\n",
      " 'closed_loop': {},\n",
      " 'connect': {},\n",
      " 'dataset': None,\n",
      " 'early_stopping': None,\n",
      " 'early_stopping_params': {},\n",
      " 'minimize_gain': {},\n",
      " 'minimizers': {'error': {'A': 'Fir2', 'B': 'SamplePart4', 'loss': 'mse'}},\n",
      " 'models': ['model'],\n",
      " 'n_samples_test': 27,\n",
      " 'n_samples_train': 27,\n",
      " 'n_samples_val': 27,\n",
      " 'name': None,\n",
      " 'num_of_epochs': 10,\n",
      " 'optimizer': 'Adam',\n",
      " 'optimizer_defaults': {'lr': 0.01},\n",
      " 'optimizer_params': [{'params': 'PFir3W'}],\n",
      " 'prediction_samples': -1,\n",
      " 'select_model': 'last',\n",
      " 'select_model_params': {},\n",
      " 'shuffle_data': True,\n",
      " 'splits': [100, 0, 0],\n",
      " 'step': 0,\n",
      " 'test_batch_size': 1,\n",
      " 'test_step': 0,\n",
      " 'test_tag': 'dataset_test',\n",
      " 'train_batch_size': 4,\n",
      " 'train_dataset': 'dataset',\n",
      " 'train_step': 0,\n",
      " 'train_tag': 'dataset',\n",
      " 'unused_samples': 3,\n",
      " 'update_per_epochs': 6,\n",
      " 'val_batch_size': 27,\n",
      " 'val_step': 0,\n",
      " 'val_tag': 'dataset_val',\n",
      " 'validation_dataset': 'dataset_val'}\n",
      "Training completed with parameters: None\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(\"Training completed with parameters:\", pprint.pprint(training_parameters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
